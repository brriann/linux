
###
### Ch. 13 - Abstraction: Address Spaces
###

-------------------------------------------
Program Code
-------------------------------------------
Heap (dynamically-allocated, user-managed memory)
||
\/ HEAP GROWS DOWN
-------------------------------------------
/\ STACK GROWS UP
||
Stack (track location in function call chain, allocate local variables, pass parameters and return values to/from routines)
-------------------------------------------

Goals:
transparency - process shouldn't be aware of the fact that memory is virtualized
efficiency - time and space, no shit
protection - isolation of memory, no inter-process address space access

Process are NOT aware of their physical address space.  They are aware of a virtual address space.
OS and hardware cooperate to translate virtual address space to physical address

###
### Ch. 14 - Memory API
###

Stack memory - allocations/deallocations handled implicitly by compiler
(Stack = automatic memory)
int x;

Heap memory - alloc/dealloc handled by programmer
int *x = (int *) malloc(sizeof(int));

malloc() - ask for size on heap. pointer to newly-allocated space is returned

free() - takes pointer

segmentation fault - forgot to allocate memory?
buffer overflow - didn't allocate enough memory?
uninitialized read - forgot to initialize allocated memory?
memory leak - forgot to free memory?
dangling pointer - freed memory before you were done with it?
double free / invalid free - multiple calls to free, calling free on some non-pointer value

TWO LEVELS OF MEMORY MANAGEMENT (aka, why no memory leaks from an exited process)
1. OS level memory management - memory handed to processes when run, and retrieved when process exits/dies
2. Memory management within Process - within Heap when malloc() and free() are called

MEMORY MGMT TOOLS
-purify
-valgrind ****

system calls within malloc() and free()
-brk - location of program's "break", aka, end of Heap
-sbrk - increments

mmap() - obtain memory from OS, anonymous memory region "swap space", not associated with any file
- this space can be managed like a Heap

###
### Ch. 15 - Mechanism, Address Translation
###

hardware-based address translation
OS and hardware cooperate to translate and manage memory

ILLUSION: each program has its own private memory, where its own code and data reside
REALITY: programs share memory, as CPUs switch between running programs

simple idea: BASE AND BOUNDS, or DYNAMIC RELOCATION
- 2 hardware registers, Base Register and Bounds/Limit Register
- OS sets Base Register "where in memory this program's address space will be located"
** physical address = virtual address + base

MMU in a CPU (Memory Management Unit)

CPU instructions to set base/bounds registers are privileged / kernel level

CPU generates EXCEPTIONS when a Process attempts access outside of its bounded address space

DATA STRUCTURE: FREE LIST - when new Process is created, OS must search for new address space
-Free List consists of open memory segments
-Free List is searched/removed from by OS on Process Start, and re-populated by OS on Process Exit/Kill

When Process stops running:
-OS must save base/bounds register values to memory, in a per-process structure
PROCESS STRUCTURE OR PCB, PROCESS CONTROL BLOCK

While Process is stopped, OS can update its base/bounds saved register values to change address space
-on startup, Process will continue as normal, unaware of change

Problem: Internal Fragmentation. lots of wasted space INSIDE an Address Space, between a Stack and Heap!


###
### Ch. 16 - Segmentation
###

Segmentation: generalized base and bounds

- base and bounds pair PER logical segment of address space (Program Code, Heap, Stack)

Segmentation Fault - out of bounds memory access

Question: what Segment are we in?
EXPLICIT SEGMENT DEFINITION: Segment, Offset
IMPLICIT: hardware notices how address was formed - from program counter? from stack/base pointer? else = Heap

STACK GROWS BACKWARDS
- more hardware support - bit, for "which way does a Segment grow"

SUPPORT FOR CODE SHARING
- code sharing between programs - memory is shared
- protection bits: mark memory segment as read-only

Fine-Grained vs Coarse-Grained Segmentation
Coarse = Code/Heap/Stack
Fine = more granular

SEGMENT TABLE - stored in memory to support many segments

OS SUPPORT
- on a context switch, OS must save/restore segment registers
- when creating new address space, OS must find space in physical memory for its segments

External Fragmentation / Non-Compacted Memory : lots of tiny slivers of wasted memory space between allocated segments

Approaches for free-list memory management:
-best-fit
-worst-fit
-first-fit
-buddy algorithms



###
### Ch. 17 - Free Space Management
###

Compaction, External Fragmentation, problem of variable-sized requests

Mechanisms:
Splitting: dividing free chunks of memory to satisfy a small request
Coalescing: merging neighboring chunks of free memory

Free List - maintaining a list of free units of memory, EMBEDDED in the memory itself
- using a HEADER block to identify size of a given block, as well as a linked list type of pointer/ memory address to "next free unit in list"

Growing The Heap of an address space: allocators start with a small heap, and make calls to "sbrk" type of system calls, adding free memory to address space of requesting process

STRATEGIES FOR MANAGING FREE SPACE

Best Fit: find memory chunks in Free List that are bigger than requested size. Choose the smallest of these.
- generally leaves SMALL chunks on the Free List

Worst Fit: find the LARGEST chunk and return the requested amount, keep remaining split of chunk on the Free List
- generally leaves BIG chunks on the Free List
- performs bad? leads to fragmentation

First Fit: find first block in Free List that satisfies request
- BENEFIT: don't have to traverse the entire Free List
- OPTIMIZE: by keeping the Free List ordered by address of free space "Address-based ordering"

Next Fit: keep pointer to Free List location where allocator was looking last
- spread searches for free space throughout the list more uniformly

Segregated Lists: maintain several Free Lists, of popular request size chunks of free memory
- forward other requests to a general memory allocator
-Slab Allocator: when kernel boots up, object caches are allocated for kernal objects likely to be frequently requested (locks, filesystem inodes, etc)
- when a given cache is low on space, SLABS of memory from a general allocator are requested

Buddy Allocation: view free memory as a big space of size 2^N
- to fill a request, recursively divide free space by 2 until a block size accomodates request
- vulnerable to internal fragmentation (can only give out power-of-2 size blocks)
- BENEFIT: when memory is freed. check for "buddy" of same size, and join them back together as possible
- 8kb free, buddy is 8kb, join to 16kb...check 16kb buddy, join if possible, etc

glibc allocator
https://www.gnu.org/software/libc/manual/html_node/The-GNU-Allocator.html


###
### Ch. 18 - Paging Introduction
###

As opposed to Segmentation,
PAGING - splits a process's address space into FIXED-SIZED UNITS called Pages
physical memory viewed as an array of fixed-size slots calle Page Frames

Page Table: a per-process data structure
- Address Translations (virtual to physical) for each virtual page of the address space are stored here

Virtual Address = VPN (Virtual Page Number) + Offset (within Page)
-now it can be translated

Which physical frame does a given Virtual Page fit into?
-Physical Frame Number PFN, or Physical Page Number PPN

Where are Page Tables stored?
- no hardware in the on-chip MMU to store page table of the currently running process
- page tables can become BIG
PTE = Page Table Entry
- page tables can be stored in virtualized OS memory, physical memory, or even swapped to disk

What's in the Page Table?
-Linear Page Table -array, indexed by VPN (Virtual Page Number)
-Bits to define characteristics of a Page:
-- Valid bit
-- Protection bit
-- Present bit
-- Dirty bit
-- Reference bit

Page Tables in memory can be too big, and too slow
- address translations must happen before Page Table is referenced?
- "Page-Table Base Register" can store physical address of starting location of Page Table


###
### Ch. 19 - Paging, Faster Translations (TLBs)
###

How can we speed up address translations?

TLB = translation-lookaside buffer
-aka, an address-translation cache

a TLB is part of the chip's MMU (memory management unit), and is a hardware cache of popular virtual-to-physical address translations

general algorithm:
1. extract the VPN (virtual page number) from a virtual address
2. check if TLB holds the translation for this VPN
3. if yes, TLB HIT, cache holds the translation
4. else, TLB MISS, hardware accesses Page Table to find the translation, and stores it in TLB

Arrays are stored contiguously in memory.
- if iterating over an array, TLB caching will benefit subsequent accesses. find a[0], then address for a[1]....etc are cached in TLB. LESS ADDRESS TRANSLATION

Spatial Locality: elements are stored closely together in physical memory
Temporal Locality: elements are quickly re-referenced in time

CISC : Complex Instruction Set Computers
RISC : Reduced Instruction Set Computers

software or hardware managed TLB? who handles the TLB miss?

Handling a CPU Context Switch with the TLB cache
-DON'T want to just erase the TLB each context switch. 
- add Address Space ID ASID to the TLB - which Process or PID does the cache entry belong to?
- Processes will also share memory, eg code

Cache Replacement Policy
- when adding new entries, which older entries are removed?
- LRU: Least-Recently Used
- Random

"RAM isn't always RAM"
- TLB can be the source of performance problems
- what if number of pages in address space exceeds TLB coverage?

###
### Ch. 20 - Paging, Smaller Tables
###

Linear Page Tables, or array-based, are simply too big, and consume too much memory

Big Pages lead to Internal Fragmentation (wasted space INSIDE a unit)

Hybrid Approach: Paging and Segments

Multi-Level Page Tables:
-turn the linear page table into a tree-like structure
- new structure: Page Directory
-- where is a Page of the Page Table stored?

Page Directory: a 2-level table
- one entry per page of the Page Table
- PDE: Page Directory Entry

TIME-SPACE TRADE OFFS
- also, COMPLEXITY

More than 2-level deep Page Directory

REMEMBER THE TLB CACHE!
- only on a TLB MISS does the Multi-Level Page Table need to be traversed/consulted

Inverted Page Tables
-single page table, eich an entry for each physical page of the system
- entry tells us which process is using this page
-- and, which virtual page of that process maps to this physical page

Swapping Page Tables to Disk
- page tables don't necessarily reside in kernel-owned physical memory.
-can be stored in Kernal Virtual Memory,
-- thus, able to be swapped to disk as needed, per "memory pressure"


###
### Ch. 21 - Beyond Physical Memory: Mechanisms
###

How can the OS make use of hard disk drive (SLOWER, larger) to provide the illusion of a larger address space?

MEMORY HIERARCHY

Swap Space - disk space reserved for moving pages of memory back and forth
-Disk Address of a given page must be tracked by OS

Present Bit - tracked in Page Table, is this Page present in physical memory, or on disk?

TLB hit/miss

Page Fault - page not in Page Table
Page Fault Handler - OS swaps page from disk into memory
- this is an IO operation - Process needing Page of memory will be BLOCKED until finished

If Memory is Full-
before a "Page In" from disk, OS might need to "Page Out" a page from memory to disk

Page Replacement Policy - defines how to Page Out, and which Page(s) to boot

High Watermark and Low Watermark
-OS tracks when fewer than LW pages are available in memory
-swap daemon or page daemon, a background thread, evicts pages until there are HW pages available

Cluster/Group pages when writing out to swap partition on disk - increases efficiency of disk


###
### Ch. 22 - Beyond Physical Memory: Policies
###

Memory Pressure - lack of free memory
- forces OS to start Paging Out pages to disk, making room for actively-used pages of memory

Replacement Policy - which memory Pages to Evict?

Main Memory can be viewed as a CACHE for virtual memory pages in the system
-goal of Replacement Policy is to minimize cache misses

AMAT - Average Memory Access Time 

AMAT = Tm + ( Pmiss * Td)
Tm - cost of accessing memory
Pmiss - probability of cache miss (not finding data in cache aka memory)
Td - cost of accessing disk

Optimal Replacement Policy - replaces the page that will be accessed FURTHEST IN THE FUTURE
-this requires future knowledge

Cache starts in empty state - Cold-Start Miss or Compulsory Miss

FIFO: Simple Policy
- pages placed in Queue, when a replacement occures, the "First-In" page is replaced

Random: Simple Policy
- randomly boot pages for a replacement

Simple Policies may kick out an important page, one that was about to be used again

Frequency, Recency - two key properties to identify important Pages

Principle of Locality - a heuristic, programs tend to access certain code/data sequences frequently (eg, loops, arrays)

LFU - Least Frequently Used
LRU - Least Recently Used

(MFU, MRU, "Most") - ignore, instead of embracing, locality that most programs exhibit. Boo!

WORKLOAD TYPES
- No-Locality workload, totally random access
- 80-20 Workload, 80% of accesses are made to 20% of memory Pages
- Looping Workload, something like 10k accesses to 50 unique pages

Approximating LRU - "clock algorithm" 
- cycle through Pages, flipping a "recently accessed" bit, until either an empty is encoutered for swap, or all are "recents", and flipped back to 0

Dirty Pages - track with Dirty Bit
-have been modified. an eviction is expensive, since changes must be persisted

What should OS do when memory is oversubcribed?
- Thrashing happens - constant paging
-admission control (throttle active processes)
-out-of-memory killer (kill a memory-intensive process)


###
### Ch. 23 - Complete Virtual Memory Systems
###

VAX/VMS
-Memory Management Hardware
-Page 0 is nullptr, "Invalid Bit" - this is how nullptr is supported in C/C++
-Page Replacement
-- segmented FIFO, per process, second-chance lists

Linux
-kernel logical addresses, vs kernel virtual addresses
-- kernel logical addresses support easy translation, and contiguous memory for IO / DMA (direct memory access)
-- kernal virtual addresses usually not contiguous

Page Table Structure - 64 bit, 48 bits used

Large Page Support
-lots of small Pages clutter up Page Table
-instead of just 4-KB pages, 2-MB and even 1-GB pages are supported for memory intensive processes
- "Huge Pages" suffer from internal fragmentation - wasted space inside the page

Page Cache - 
- memory mapped files
- file data / device metadata
- heap/stack pages comprising each process - ANONYMOUS MEMORY (no named file underneath, rather, just swap space)

Linux 2Q Page Replacement Policy
- similar to LRU, but with 2 lists, Inactive List and Active List
- Pages promoted to Active List when re-referenced

Security, Buffer Overflow Protection
- don't let arbitrary data be injected into kernel address space!
- this can result in privilege escalation (user code gaining kernel access rights)

return-to-libc attack

Defense: ASLR, Address Space Layout Randomization
- obscure the locations of code / heap / stack within virtual address spaces
- this can be observed by re-running code and seeing pointer locations, they should change (eg, a stack int variable)

KASLR - Kernel Address Space Layout Randomization

PROBLEMS FOR KERNEL:

Meltdown, Spectre

Speculative Execution (predictive optimization behavior of CPU) can leave traces in things like processsor caches, branch predictors, etc
- this type of state can leave parts of the memory vulnerable

Kernel Page-Table Isolation

*******
vmstat utility - reports information about processes, memory, paging, block IO, traps, and cpu activity.
*******

https://www.redhat.com/sysadmin/linux-commands-vmstat
https://linux.die.net/man/8/vmstat

